{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ca893e-fec7-4292-bee6-ecdf634f4404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.12/site-packages (3.1.5)\n",
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Installing collected packages: xlrd\n",
      "Successfully installed xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl xlrd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fb8df27-b303-4308-91d7-3d771c36560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydicom\n",
      "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydicom\n",
      "Successfully installed pydicom-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77fb8089-3d77-4651-87ec-c2c92b230d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Image Data ID     Subject Group Sex  Age Visit Modality Description  \\\n",
      "0       I789309  098_S_0269   MCI   M   72   v06      MRI     MP-RAGE   \n",
      "1       I711042  116_S_1249    CN   F   80   v41      MRI     MP-RAGE   \n",
      "2       I665351  116_S_1232    CN   F   81   v41      MRI     MP-RAGE   \n",
      "3       I664578  007_S_1222    CN   F   83   v41      MRI     MP-RAGE   \n",
      "4       I493363  116_S_0361   MCI   M   84   v41      MRI     MP-RAGE   \n",
      "\n",
      "       Type   Acq Date Format Downloaded  \n",
      "0  Original  4/02/2011    DCM  1/18/2025  \n",
      "1  Original  5/19/2016    DCM  1/18/2025  \n",
      "2  Original  3/30/2016    DCM  1/18/2025  \n",
      "3  Original  2/08/2016    DCM  1/18/2025  \n",
      "4  Original  5/18/2015    DCM  1/18/2025  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2037 entries, 0 to 2036\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Image Data ID  2037 non-null   object\n",
      " 1   Subject        2037 non-null   object\n",
      " 2   Group          2037 non-null   object\n",
      " 3   Sex            2037 non-null   object\n",
      " 4   Age            2037 non-null   int64 \n",
      " 5   Visit          2037 non-null   object\n",
      " 6   Modality       2037 non-null   object\n",
      " 7   Description    2037 non-null   object\n",
      " 8   Type           2037 non-null   object\n",
      " 9   Acq Date       2037 non-null   object\n",
      " 10  Format         2037 non-null   object\n",
      " 11  Downloaded     2037 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 191.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = \"~/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset/projectMRI_1_18_2025.csv\"\n",
    "metadata = pd.read_csv(excel_path)\n",
    "\n",
    "# Inspect the first few rows\n",
    "print(metadata.head())\n",
    "print(metadata.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e9bb1c-631f-492f-9e83-8390913b587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data ID    0\n",
      "Subject          0\n",
      "Group            0\n",
      "Sex              0\n",
      "Age              0\n",
      "Visit            0\n",
      "Acq Date         0\n",
      "dtype: int64\n",
      "  Image Data ID     Subject  Group Sex  Age Visit   Acq Date\n",
      "0       I789309  098_S_0269      1   M   72   v06  4/02/2011\n",
      "1       I711042  116_S_1249      0   F   80   v41  5/19/2016\n",
      "2       I665351  116_S_1232      0   F   81   v41  3/30/2016\n",
      "3       I664578  007_S_1222      0   F   83   v41  2/08/2016\n",
      "4       I493363  116_S_0361      1   M   84   v41  5/18/2015\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "metadata = metadata.drop(columns=[\"Modality\", \"Description\", \"Type\", \"Format\", \"Downloaded\"])\n",
    "\n",
    "# Check for missing values\n",
    "print(metadata.isnull().sum())\n",
    "\n",
    "# Encode the Group column\n",
    "group_mapping = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}  # Example mapping\n",
    "metadata[\"Group\"] = metadata[\"Group\"].map(group_mapping)\n",
    "\n",
    "print(metadata.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940aec42-44f3-4828-a410-7227876b4cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects in metadata: ['098_S_0269' '116_S_1249' '116_S_1232' '007_S_1222' '116_S_0361'\n",
      " '116_S_0382' '037_S_0588' '130_S_0285' '126_S_0709' '126_S_0606'\n",
      " '037_S_0377' '094_S_1330' '094_S_0531' '053_S_0389' '137_S_0668'\n",
      " '052_S_1250' '052_S_1346' '052_S_1352' '130_S_0969' '014_S_0520'\n",
      " '014_S_0519' '130_S_0505' '006_S_0498' '006_S_0731' '116_S_0752'\n",
      " '098_S_0667' '052_S_0671' '052_S_0989' '052_S_0951' '052_S_0952'\n",
      " '037_S_0467' '098_S_0171' '098_S_0172' '130_S_0289' '130_S_0232'\n",
      " '037_S_0501' '006_S_1130' '116_S_1271' '116_S_1243' '098_S_0160'\n",
      " '098_S_0896' '037_S_0454' '037_S_1078' '005_S_0610' '130_S_0886'\n",
      " '005_S_0546' '005_S_0553' '053_S_0919' '037_S_0150' '037_S_0552'\n",
      " '037_S_0303' '136_S_0186' '131_S_0441' '027_S_0408' '027_S_0403'\n",
      " '027_S_0307' '131_S_0384' '027_S_0256' '131_S_0123' '052_S_1251'\n",
      " '014_S_0169' '027_S_1387' '029_S_1218' '005_S_0602' '027_S_0116'\n",
      " '027_S_0120' '024_S_1063' '027_S_0118' '127_S_0112' '029_S_0878'\n",
      " '029_S_0914' '007_S_0068' '002_S_1155' '027_S_0074' '137_S_0972'\n",
      " '127_S_1032' '137_S_0994' '009_S_1030' '021_S_0984' '137_S_0973'\n",
      " '024_S_0985' '137_S_0722' '009_S_0842' '029_S_0845' '033_S_1016'\n",
      " '033_S_1098' '127_S_0925' '027_S_1045' '116_S_0834' '136_S_0873'\n",
      " '033_S_0922' '033_S_0923' '033_S_0920' '033_S_0906' '014_S_0563'\n",
      " '116_S_0657' '037_S_1421' '029_S_0824' '094_S_1417' '137_S_0800'\n",
      " '027_S_0835' '007_S_0698' '014_S_0558' '127_S_1427' '137_S_1414'\n",
      " '127_S_1419' '021_S_0626' '033_S_0734' '033_S_0741' '014_S_0548'\n",
      " '014_S_0658' '002_S_0729' '126_S_0680' '002_S_0685' '126_S_0708'\n",
      " '127_S_0622' '131_S_1389' '126_S_0605' '136_S_0107' '027_S_0644'\n",
      " '136_S_0086' '037_S_0539' '007_S_0344' '033_S_0514' '002_S_0295'\n",
      " '029_S_1318' '002_S_0413' '021_S_0424' '021_S_0337' '029_S_1384'\n",
      " '127_S_0260' '007_S_0249' '024_S_1393' '005_S_1224' '021_S_0276'\n",
      " '116_S_1315' '136_S_1227' '094_S_1314' '127_S_0259' '094_S_1267'\n",
      " '021_S_0159' '129_S_1246' '002_S_1268' '131_S_1301' '002_S_1261'\n",
      " '007_S_0128' '021_S_0141' '094_S_1241' '002_S_1280' '094_S_1188'\n",
      " '007_S_1206' '130_S_1200' '007_S_0101' '094_S_1293' '136_S_0874'\n",
      " '029_S_1073' '007_S_0041' '127_S_1140' '126_S_0865' '126_S_1077'\n",
      " '002_S_1070' '037_S_0566' '029_S_0843' '029_S_0866' '009_S_0862'\n",
      " '094_S_0921' '136_S_0695' '129_S_0778' '002_S_0782' '009_S_0751'\n",
      " '094_S_0711' '033_S_0723' '137_S_0669' '116_S_0649' '137_S_0443'\n",
      " '116_S_0648' '137_S_0686' '136_S_0429' '137_S_0631' '005_S_0572'\n",
      " '133_S_0525' '002_S_0559' '094_S_0526' '094_S_0434' '037_S_0327'\n",
      " '053_S_0621' '033_S_0567' '033_S_0516' '127_S_0394' '137_S_0481'\n",
      " '133_S_0488' '033_S_0513' '005_S_0324' '133_S_0433' '053_S_0507'\n",
      " '133_S_0493' '007_S_1339' '005_S_0223' '137_S_0459' '136_S_0196'\n",
      " '137_S_0301' '137_S_0283' '127_S_1382' '007_S_0293' '021_S_0273'\n",
      " '136_S_0195' '005_S_1341' '137_S_0158' '136_S_0184' '027_S_1385'\n",
      " '133_S_1170' '027_S_0179' '033_S_1279' '027_S_1254' '033_S_1281'\n",
      " '033_S_1308' '033_S_1309' '033_S_1283' '033_S_1285' '024_S_1307'\n",
      " '137_S_0796' '126_S_1187' '094_S_1164' '126_S_1221' '029_S_1215'\n",
      " '027_S_1081' '027_S_1213' '024_S_1171' '029_S_1056' '137_S_1041'\n",
      " '027_S_1082' '002_S_1018' '006_S_0675' '053_S_1044' '052_S_1054'\n",
      " '133_S_1055' '033_S_1086' '094_S_1398' '052_S_1168' '127_S_0844'\n",
      " '133_S_1031' '094_S_0692' '133_S_0912' '002_S_0938' '137_S_1426'\n",
      " '006_S_0681' '027_S_0850' '033_S_0889' '005_S_0814' '021_S_0753'\n",
      " '127_S_0754' '133_S_0771' '133_S_0727' '126_S_0784' '126_S_0891'\n",
      " '133_S_0638' '133_S_0792' '006_S_0547' '037_S_1225' '021_S_0231'\n",
      " '033_S_0739' '021_S_0647' '136_S_0579' '002_S_0619' '033_S_0724'\n",
      " '033_S_0725' '033_S_0733' '133_S_0913' '131_S_0497' '014_S_0557'\n",
      " '021_S_1109' '127_S_0431' '031_S_0618' '031_S_0554' '033_S_1116'\n",
      " '131_S_0457' '136_S_0299' '137_S_0366' '136_S_0426' '007_S_0414'\n",
      " '031_S_1066' '031_S_0568' '033_S_0511' '116_S_0487' '116_S_0392'\n",
      " '002_S_0954' '005_S_0448' '094_S_1402' '007_S_0316' '136_S_0300'\n",
      " '094_S_1397' '132_S_0987' '027_S_0404' '116_S_0370' '014_S_0328'\n",
      " '021_S_0343' '005_S_0221' '031_S_0351' '031_S_0867' '031_S_0294'\n",
      " '005_S_0222' '031_S_0830' '029_S_0999' '033_S_1284' '027_S_1277'\n",
      " '031_S_1209' '014_S_1095' '002_S_0816' '098_S_0149' '094_S_1090'\n",
      " '027_S_0461' '127_S_0393' '024_S_1400' '094_S_1027' '027_S_0485'\n",
      " '137_S_0825' '129_S_1204' '029_S_0836' '007_S_1304' '037_S_0627'\n",
      " '127_S_0684' '126_S_1340' '116_S_0890' '133_S_0629' '021_S_0642'\n",
      " '094_S_1102' '029_S_1038' '116_S_1083' '027_S_0417' '094_S_1015'\n",
      " '002_S_0955' '005_S_0929' '137_S_0841' '021_S_0332' '031_S_0321'\n",
      " '031_S_0821' '136_S_0194' '006_S_0484' '007_S_0070' '027_S_1335'\n",
      " '131_S_0691' '131_S_0409' '127_S_1210' '007_S_1248' '029_S_1184'\n",
      " '126_S_0405' '029_S_0871' '098_S_0884' '127_S_0397' '006_S_0653'\n",
      " '094_S_0489' '033_S_1087' '014_S_0356' '131_S_0319' '027_S_0948'\n",
      " '094_S_0964' '033_S_0888' '130_S_0783' '031_S_0773' '130_S_0102'\n",
      " '128_S_0740' '128_S_0770' '128_S_0715' '128_S_0701' '006_S_0521'\n",
      " '006_S_0322' '128_S_0611' '128_S_0608' '130_S_0449' '130_S_0423'\n",
      " '128_S_0517' '128_S_0545' '098_S_0542' '128_S_0522' '130_S_0460'\n",
      " '131_S_0436' '128_S_0528' '128_S_0500' '116_S_0360' '132_S_0339'\n",
      " '137_S_0438' '128_S_0258' '014_S_0357' '098_S_0288' '128_S_0229'\n",
      " '128_S_0310' '128_S_0272' '128_S_0266' '128_S_0245' '128_S_0227'\n",
      " '128_S_0230' '128_S_0167' '128_S_0225' '037_S_0182' '128_S_0200'\n",
      " '128_S_0216' '128_S_0205' '021_S_0178' '128_S_0188' '128_S_0138'\n",
      " '128_S_0135']\n",
      "Folders in DICOM directory: ['128_S_0258', '027_S_1254', '031_S_0351', '094_S_1102', '002_S_0782', '094_S_1330', '052_S_0951', '014_S_0558', '130_S_0505', '007_S_0101', '005_S_0223', '128_S_0205', '094_S_1397', '136_S_0695', '137_S_0800', '037_S_0467', '133_S_0488', '127_S_1032', '029_S_0836', '021_S_0647', '007_S_1248', '031_S_0554', '029_S_1184', '005_S_0222', '005_S_0814', '137_S_0631', '014_S_0557', '021_S_0276', '007_S_1222', '027_S_0179', '128_S_0266', '021_S_1109', '126_S_0784', '127_S_0112', '098_S_0288', '130_S_0102', '029_S_1318', '094_S_1398', '002_S_1268', '137_S_0459', '014_S_0169', '006_S_0731', '002_S_1261', '024_S_0985', '029_S_1056', '033_S_1285', '033_S_0733', '033_S_0734', '128_S_0188', '137_S_0973', '009_S_0751', '128_S_0715', '033_S_1086', '130_S_0289', '006_S_0681', '053_S_0621', '027_S_1387', '006_S_0675', '027_S_0403', '126_S_0605', '136_S_0186', '027_S_0404', '.DS_Store', '126_S_1340', '007_S_1304', '128_S_0770', '116_S_1083', '037_S_0327', '007_S_0249', '128_S_0545', '132_S_0339', '002_S_0295', '027_S_0461', '130_S_0449', '021_S_0332', '116_S_0752', '094_S_1015', '128_S_0528', '126_S_1187', '024_S_1063', '033_S_1279', '128_S_0517', '033_S_1087', '127_S_1140', '130_S_0886', '024_S_1400', '006_S_0484', '033_S_1283', '005_S_1224', '130_S_0423', '137_S_0972', '033_S_1284', '037_S_0182', '005_S_0553', '052_S_0671', '033_S_1016', '133_S_0792', '128_S_0740', '098_S_0160', '116_S_1243', '094_S_0964', '037_S_0552', '130_S_0232', '094_S_1090', '131_S_0691', '094_S_0526', '037_S_1225', '037_S_0303', '131_S_0457', '031_S_0830', '033_S_0920', '127_S_0684', '002_S_0619', '136_S_1227', '027_S_1335', '094_S_1267', '002_S_0413', '094_S_1293', '116_S_0382', '136_S_0194', '005_S_0929', '131_S_1389', '128_S_0167', '137_S_0366', '007_S_0698', '133_S_0525', '014_S_0658', '033_S_0513', '037_S_0501', '037_S_0539', '002_S_0816', '033_S_0514', '116_S_1232', '021_S_0141', '007_S_0068', '116_S_0370', '029_S_1215', '136_S_0300', '128_S_0135', '098_S_0172', '126_S_0680', '137_S_0994', '005_S_0546', '037_S_1421', '098_S_0542', '133_S_0727', '021_S_0178', '128_S_0701', '137_S_1426', '006_S_0498', '029_S_1073', '027_S_0417', '021_S_0343', '136_S_0195', '129_S_1246', '127_S_0622', '027_S_0074', '031_S_0773', '137_S_0443', '007_S_0344', '021_S_0231', '006_S_0521', '002_S_0954', '126_S_1221', '031_S_1209', '029_S_0824', '009_S_0862', '094_S_1188', '024_S_1307', '014_S_0519', '131_S_0319', '031_S_0321', '133_S_0493', '137_S_0481', '127_S_1419', '133_S_0433', '116_S_0657', '137_S_0283', '116_S_1315', '137_S_0841', '031_S_1066', '027_S_1277', '027_S_1045', '128_S_0611', '126_S_0708', '127_S_0393', '098_S_0667', '116_S_0834', '128_S_0272', '094_S_1314', '127_S_0394', '002_S_0938', '014_S_0520', '128_S_0229', '002_S_1280', '130_S_0783', '128_S_0216', '131_S_0123', '002_S_1070', '098_S_0269', '002_S_0955', '027_S_1213', '128_S_0227', '137_S_0825', '027_S_1081', '029_S_0871', '007_S_0316', '002_S_0559', '126_S_0709', '006_S_0322', '133_S_0638', '007_S_1206', '029_S_0878', '094_S_0692', '136_S_0874', '128_S_0245', '052_S_0989', '126_S_1077', '136_S_0873', '133_S_1170', '127_S_1427', '137_S_0669', '127_S_0925', '005_S_0610', '033_S_1308', '128_S_0230', '031_S_0568', '014_S_0357', '094_S_1164', '098_S_0884', '009_S_0842', '021_S_0273', '029_S_0866', '116_S_0648', '033_S_0889', '116_S_0487', '021_S_0626', '014_S_0563', '053_S_1044', '052_S_0952', '136_S_0299', '002_S_0729', '027_S_0116', '021_S_0642', '033_S_1309', '014_S_0356', '037_S_0454', '027_S_0118', '128_S_0200', '137_S_0668', '027_S_0120', '027_S_0948', '094_S_0489', '005_S_0448', '033_S_0888', '128_S_0608', '053_S_0507', '005_S_0221', '133_S_0629', '024_S_1171', '116_S_0649', '021_S_0424', '029_S_1384', '130_S_0969', '137_S_0438', '128_S_0310', '033_S_0567', '129_S_0778', '136_S_0579', '116_S_0360', '031_S_0821', '037_S_0588', '131_S_0441', '006_S_1130', '126_S_0865', '033_S_1281', '126_S_0891', '127_S_0260', '031_S_0618', '127_S_0431', '052_S_1168', '137_S_0722', '126_S_0405', '033_S_0739', '037_S_0377', '027_S_0256', '126_S_0606', '021_S_0159', '116_S_1249', '029_S_1038', '021_S_0337', '021_S_0753', '094_S_0531', '094_S_1417', '094_S_1241', '007_S_0414', '007_S_0070', '116_S_1271', '033_S_0906', '027_S_0835', '116_S_0361', '129_S_1204', '131_S_1301', '029_S_0914', '007_S_0041', '116_S_0392', '027_S_1385', '136_S_0184', '007_S_1339', '127_S_1382', '052_S_1352', '130_S_0285', '128_S_0522', '127_S_0259', '094_S_1027', '027_S_0850', '027_S_0408', '021_S_0984', '131_S_0409', '031_S_0294', '131_S_0436', '002_S_0685', '037_S_0150', '005_S_0324', '005_S_0572', '052_S_1346', '128_S_0500', '009_S_1030', '037_S_1078', '033_S_0725', '133_S_1055', '127_S_0844', '031_S_0867', '130_S_0460', '033_S_0923', '133_S_0771', '133_S_0912', '002_S_1155', '007_S_0293', '033_S_1098', '137_S_0158', '033_S_0511', '033_S_0723', '137_S_1041', '006_S_0653', '033_S_0724', '033_S_0516', '136_S_0196', '029_S_0999', '137_S_1414', '132_S_0987', '094_S_0921', '033_S_0741', '133_S_1031', '131_S_0497', '037_S_0566', '133_S_0913', '137_S_0796', '094_S_0711', '128_S_0138', '027_S_0485', '098_S_0171', '029_S_1218', '094_S_1402', '137_S_0301', '027_S_0644', '033_S_0922', '098_S_0149', '136_S_0107', '052_S_1250', '029_S_0845', '053_S_0389', '136_S_0086', '007_S_0128', '006_S_0547', '136_S_0429', '127_S_0397', '052_S_1054', '094_S_0434', '127_S_0754', '116_S_0890', '137_S_0686', '098_S_0896', '128_S_0225', '005_S_0602', '024_S_1393', '002_S_1018', '136_S_0426', '014_S_0328', '027_S_1082', '037_S_0627', '029_S_0843', '014_S_0548', '052_S_1251', '131_S_0384', '027_S_0307', '053_S_0919', '005_S_1341', '014_S_1095', '127_S_1210', '033_S_1116', '130_S_1200']\n",
      "  Image Data ID     Subject  Group Sex  Age Visit   Acq Date  \\\n",
      "0       I789309  098_S_0269      1   M   72   v06  4/02/2011   \n",
      "1       I711042  116_S_1249      0   F   80   v41  5/19/2016   \n",
      "2       I665351  116_S_1232      0   F   81   v41  3/30/2016   \n",
      "3       I664578  007_S_1222      0   F   83   v41  2/08/2016   \n",
      "4       I493363  116_S_0361      1   M   84   v41  5/18/2015   \n",
      "\n",
      "                                           FilePaths  \n",
      "0  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "1  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "2  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "3  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "4  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n"
     ]
    }
   ],
   "source": [
    "# Get all the file paths per user and create a new column on the metadata dataset called filepaths\n",
    "\n",
    "import os\n",
    "# Path to your DICOM directory\n",
    "dicom_dir = os.path.expanduser(\"~/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset/ADNI2\")\n",
    "\n",
    "# Add file paths to metadata\n",
    "def get_file_path(subject_id):\n",
    "    \"\"\"Retrieve all .dcm files for a given subject.\"\"\"\n",
    "    for root, dirs, files in os.walk(dicom_dir):  # Recursively walk through directories\n",
    "        if subject_id in root:  # Check if subject_id is part of the current path\n",
    "            dicom_files = [os.path.join(root, f) for f in files if f.endswith(\".dcm\")]\n",
    "            if dicom_files:\n",
    "                return dicom_files  # Return all .dcm files for this subject\n",
    "    return None\n",
    "\n",
    "# Debugging: Print all subjects and DICOM directory content\n",
    "print(\"Subjects in metadata:\", metadata[\"Subject\"].unique())\n",
    "print(\"Folders in DICOM directory:\", os.listdir(dicom_dir))\n",
    "\n",
    "# Apply function to find paths\n",
    "metadata[\"FilePaths\"] = metadata[\"Subject\"].apply(get_file_path)\n",
    "\n",
    "# Drop rows with missing paths\n",
    "metadata = metadata.dropna(subset=[\"FilePaths\"])\n",
    "print(metadata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3983019-4ac2-49f5-a986-72c61009cb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Image Data ID     Subject  Group Sex  Age Visit   Acq Date  \\\n",
      "0       I789309  098_S_0269      1   M   72   v06  4/02/2011   \n",
      "1       I711042  116_S_1249      0   F   80   v41  5/19/2016   \n",
      "2       I665351  116_S_1232      0   F   81   v41  3/30/2016   \n",
      "3       I664578  007_S_1222      0   F   83   v41  2/08/2016   \n",
      "4       I493363  116_S_0361      1   M   84   v41  5/18/2015   \n",
      "\n",
      "                                           FilePaths  \n",
      "0  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "1  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "2  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "3  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "4  [/Users/carlos_mac/Documents/caoa/Lakehead/Fin...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2037 entries, 0 to 2036\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Image Data ID  2037 non-null   object\n",
      " 1   Subject        2037 non-null   object\n",
      " 2   Group          2037 non-null   int64 \n",
      " 3   Sex            2037 non-null   object\n",
      " 4   Age            2037 non-null   int64 \n",
      " 5   Visit          2037 non-null   object\n",
      " 6   Acq Date       2037 non-null   object\n",
      " 7   FilePaths      2037 non-null   object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 127.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(metadata.head())\n",
    "print(metadata.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c145f4-148f-41cd-9979-3cb6618de144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Subject                                       SelectedFile\n",
      "0  098_S_0269  /Users/carlos_mac/Documents/caoa/Lakehead/Fina...\n",
      "1  116_S_1249  /Users/carlos_mac/Documents/caoa/Lakehead/Fina...\n",
      "2  116_S_1232  /Users/carlos_mac/Documents/caoa/Lakehead/Fina...\n",
      "3  007_S_1222  /Users/carlos_mac/Documents/caoa/Lakehead/Fina...\n",
      "4  116_S_0361  /Users/carlos_mac/Documents/caoa/Lakehead/Fina...\n"
     ]
    }
   ],
   "source": [
    "#Select only one mri image per subject and add it to a new column called select file\n",
    "\n",
    "def select_dicom_file(dicom_files):\n",
    "    if dicom_files:\n",
    "        return dicom_files[0]  \n",
    "    return None\n",
    "\n",
    "# Apply selection logic\n",
    "metadata[\"SelectedFile\"] = metadata[\"FilePaths\"].apply(select_dicom_file)\n",
    "\n",
    "# Drop rows without a selected file\n",
    "metadata = metadata.dropna(subset=[\"SelectedFile\"])\n",
    "\n",
    "# Verify the selected files\n",
    "print(metadata[[\"Subject\", \"SelectedFile\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf7b22a7-b6a2-4a00-a68f-d5de9826bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2037 entries, 0 to 2036\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Subject       2037 non-null   object\n",
      " 1   SelectedFile  2037 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 32.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(metadata[[\"Subject\", \"SelectedFile\"]].info())\n",
    "\n",
    "#output_dir = os.path.expanduser(\"~/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset/MRI_3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf91ee-3cd0-439d-900d-8af59a42edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pydicom\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "# def convert_dicom_to_png(dicom_path, save_path):\n",
    "#     \"\"\"Convert a DICOM file to a PNG image.\"\"\"\n",
    "#     ds = pydicom.dcmread(dicom_path)\n",
    "#     image = ds.pixel_array  # Extract pixel data\n",
    "#     normalized_image = (image - np.min(image)) / (np.max(image) - np.min(image))  # Normalize\n",
    "#     Image.fromarray((normalized_image * 255).astype(np.uint8)).save(save_path)\n",
    "\n",
    "# # Directory to save the PNG images\n",
    "# output_dir = os.path.expanduser(\"~/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset/MRI_3D\")\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Convert all `.dcm` files in `FilePaths`\n",
    "# for index, row in metadata.iterrows():\n",
    "#     dicom_files = row[\"FilePaths\"]  # List of `.dcm` files\n",
    "#     subject_id = row[\"Subject\"]\n",
    "    \n",
    "#     for i, dicom_file in enumerate(dicom_files):\n",
    "#         save_path = os.path.join(output_dir, f\"{subject_id}_{i}.png\")  # Unique filename\n",
    "#         convert_dicom_to_png(dicom_file, save_path)\n",
    "#         print(f\"Processed {dicom_file} -> {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86deff73-ff69-4b75-831e-8391e77ccb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERTING IMAGES TO PNG FROM DICOM\n",
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Function to convert a DICOM file to a PNG image\n",
    "def convert_dicom_to_png(dicom_path, save_path):\n",
    "    \"\"\"Convert a DICOM file to a PNG image.\"\"\"\n",
    "    ds = pydicom.dcmread(dicom_path)\n",
    "    image = ds.pixel_array  # Extract pixel data\n",
    "    normalized_image = (image - np.min(image)) / (np.max(image) - np.min(image))  # Normalize (0,1)\n",
    "    Image.fromarray((normalized_image * 255).astype(np.uint8)).save(save_path) # remove floating values and convert to image from array\n",
    "\n",
    "# Directory to save the PNG images\n",
    "output_dir = os.path.expanduser(\"~/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset/MRI_3D\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clean output folder\n",
    "def clean_output_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    else:\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "clean_output_folder(output_dir)\n",
    "\n",
    "# Process files in batches\n",
    "batch_size = 10  # Process 10 subjects at a time\n",
    "metadata_batches = [metadata[i:i + batch_size] for i in range(0, len(metadata), batch_size)]\n",
    "\n",
    "# Process each batch with a progress bar\n",
    "for batch_index, batch in enumerate(metadata_batches):\n",
    "    print(f\"\\nProcessing batch {batch_index + 1}/{len(metadata_batches)}...\")\n",
    "    for _, row in tqdm(batch.iterrows(), total=len(batch), desc=f\"Batch {batch_index + 1} Progress\"):\n",
    "        dicom_files = row[\"FilePaths\"]\n",
    "        subject_id = row[\"Subject\"]\n",
    "\n",
    "        for i, dicom_file in enumerate(dicom_files):\n",
    "            save_path = os.path.join(output_dir, f\"{subject_id}_{i}.png\")\n",
    "            convert_dicom_to_png(dicom_file, save_path)\n",
    "\n",
    "    # Add a small delay to release memory after each batch\n",
    "    time.sleep(5)\n",
    "\n",
    "# Verify total number of saved images\n",
    "output_images = os.listdir(output_dir)\n",
    "print(f\"\\nProcessing complete. Total images saved: {len(output_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5900175a-2602-419c-a80c-afbfbd128b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA SPLITTING\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define dataset directories\n",
    "base_dir = os.path.expanduser(\"~/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset\")\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# Source directory for images\n",
    "source_images_dir = os.path.expanduser(\"~/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset/MRI_3D\")\n",
    "\n",
    "# Create subdirectories for each class\n",
    "for split in [train_dir, val_dir, test_dir]:\n",
    "    for group in metadata[\"Group\"].unique():\n",
    "        os.makedirs(os.path.join(split, str(group)), exist_ok=True)\n",
    "\n",
    "# Split the data into train, val, and test sets\n",
    "train_metadata, test_metadata = train_test_split(metadata, test_size=0.2, stratify=metadata[\"Group\"], random_state=42)\n",
    "train_metadata, val_metadata = train_test_split(train_metadata, test_size=0.25, stratify=train_metadata[\"Group\"], random_state=42)\n",
    "\n",
    "# Function to copy images to respective directories\n",
    "def copy_images(metadata_split, target_dir):\n",
    "    for index, row in metadata_split.iterrows():\n",
    "        # Adjust image name based on naming convention\n",
    "        image_name = f\"{row['Subject']}_0.png\"  # Use '_0.png' if multiple images per subject\n",
    "        source_path = os.path.join(source_images_dir, image_name)\n",
    "        target_path = os.path.join(target_dir, str(row[\"Group\"]), image_name)\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy(source_path, target_path)\n",
    "        else:\n",
    "            print(f\"Image not found: {source_path}\")  # Debugging output for missing images\n",
    "\n",
    "# Copy images to train, val, and test directories\n",
    "copy_images(train_metadata, train_dir)\n",
    "copy_images(val_metadata, val_dir)\n",
    "copy_images(test_metadata, test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96f72a06-661b-4fe8-9898-cd5b4478297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group\n",
      "1    1099\n",
      "0     616\n",
      "2     322\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(metadata[\"Group\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc19066-aa9d-4777-a317-8cb2b0d35c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182346a0-c9f8-48ea-a29a-c534db604637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define image transformations\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to the input size of ViT\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageFolder(train_dir, transform=image_transforms)\n",
    "val_dataset = ImageFolder(val_dir, transform=image_transforms)\n",
    "test_dataset = ImageFolder(test_dir, transform=image_transforms)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0929e7e0-ee01-4db6-a94b-21602c31920d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "import torch\n",
    "\n",
    "# Load the pretrained Vision Transformer\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=3,  # Set the number of classes to 3\n",
    "    ignore_mismatched_sizes=True  # Ignore mismatched layers\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fef19fe9-c43b-4aa3-a5da-e549f8321984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)  # Adjust learning rate as needed\n",
    "\n",
    "# Define loss function\n",
    "criterion = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08740bb9-3da0-4149-8da9-50cff538f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10  # Number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_progress = tqdm(train_loader, desc=\"Training\", leave=False)  # Progress bar for training\n",
    "    for images, labels in train_progress:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})  # Update progress bar\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_progress = tqdm(val_loader, desc=\"Validating\", leave=False)  # Progress bar for validation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_progress:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(pixel_values=images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            val_progress.set_postfix({\"Val Loss\": f\"{loss.item():.4f}\"})  # Update progress bar\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41cbb184-ecd5-4a76-b608-00f610b35d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9565\n",
      "Precision: 0.9563\n",
      "F1-Score: 0.9563\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96        70\n",
      "           1       0.96      0.97      0.97       137\n",
      "           2       0.93      0.89      0.91        46\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.94      0.95       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.numpy()  # Convert labels to NumPy format for metrics\n",
    "        outputs = model(pixel_values=images).logits\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average=\"weighted\")  # Weighted for class imbalance\n",
    "f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a78abaf7-50d3-432c-a537-4f84ee0a508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae8a752e55040d59dcb49d7c919228e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and feature extractor saved to /Users/carlos_mac/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "save_path = os.path.expanduser(\"~/Documents/caoa/Lakehead/Final_Project/Research_Papers/ADNI_Dataset\")\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "# Save the feature extractor or tokenizer if applicable\n",
    "# For example, if you're using a Vision Transformer from Hugging Face:\n",
    "from transformers import ViTFeatureExtractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "feature_extractor.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and feature extractor saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e864c-1961-43f8-8cc5-deb9a2e20b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
